{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Project 01\n",
    "\n",
    "### Checkpoint Due Date: Thursday April 9, 11:59:59 PM (Questions 1-4)\n",
    "### Due Date: Thursday, April 16, 11:59:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Instructions\n",
    "\n",
    "This Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems.  \n",
    "* Like the lab, your coding work will be developed in the accompanying `project01.py` file, that will be imported into the current notebook. This code will be autograded.\n",
    "* **For the checkpoint, turn in questions 1-4**\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are **encouraged to write your own additional functions** to solve the questions! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `project01.py` -- however, be sure to upload these to gradescope as well!\n",
    "- Always document your code!\n",
    "\n",
    "**Tips for testing the correctness of your answers!**\n",
    "Once you have your work saved in the .py file, you should import the `project01` to test your function out in the notebook. In the notebook you should inspect/analyze the output to assess its correctness!\n",
    "* Run your functions on the main dataset (`grades`) and ask yourself if the output *looks correct.*\n",
    "* Run your functions on very small datasets (e.g. 1-5 row table), calculate the expected response by hand, and see if the function output matches (this *is* unit-testing your code with data).\n",
    "* Run your functions on (large and small) samples of the dataset `grades` (with and without replacement). Does your code break? Or does it still run as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project01 as proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Other Side of Gradescope\n",
    "\n",
    "The file contains the grade-book from a fictional data science course with 535 students. \n",
    "\n",
    "**Note: this dataset is synthetically generated; it does not contain real student grades.**\n",
    "\n",
    "In this project, you will:\n",
    "1. clean and process the data to compute total course grades according to a fictional syllabus (below),\n",
    "2. qualitatively understand how students did in the course,\n",
    "3. understand how student grades vary with small changes in performance on each assignment.\n",
    "\n",
    "---\n",
    "\n",
    "The course syllabus is as follows:\n",
    "\n",
    "* Lab assignments \n",
    "    - Each are worth the same amount, regardless of each lab's raw point total.\n",
    "    - The lowest lab is dropped.\n",
    "    - Each lab may be revised for one week after submission for a 10% penalty, for two weeks after submission for a 20% penalty, and beyond that for a 50% penalty. Such revisions are reflected in the `Lateness` columns in the gradebook.\n",
    "    - Labs are 20% of the total grade.\n",
    "* Projects \n",
    "    - Each project consists of an autograded portion, and *possibly* a free response portion.\n",
    "    - The total points for a single project consist of the sum of the raw score of the two portions.\n",
    "    - Each are worth the same amount, regardless of each project's raw point total.\n",
    "    - Projects are 30% of the total grade.\n",
    "* Checkpoints\n",
    "    - Project checkpoints are worth 2.5% of the total grade.\n",
    "* Discussion\n",
    "    - Discussion notebooks are worth 2.5% of the total grade.\n",
    "* Exams\n",
    "    - The midterm is worth 15% of the total grade.\n",
    "    - The final is worth 30% of the total grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on generalization\n",
    "\n",
    "You may assume that your code will only need to work on a gradebook for a class with the syllabus given above. That is, you may assume that the dataframe `grades` looks like the given one in `data/grades.csv`.\n",
    "\n",
    "However, such a class:\n",
    "1. may have a different numbers of labs, projects, discussions, and project checkpoints.\n",
    "2. may have a different number of students.\n",
    "\n",
    "You may assume the course components and the naming conventions are as given in the data file.\n",
    "\n",
    "The dataset was generated by Gradescope; you must attempt to reason about the data as given using what you know as a student who uses Gradescope.\n",
    "\n",
    "### A note on 'putting everything together'\n",
    "\n",
    "The goal of this project is to create and assess final grades for a fictional course; if anything, the process is broken down into functions for your convenience and guidance. Here are a few remarks and tips for approaching the projects:\n",
    "1. If you are having trouble figuring out what a question is asking you to do, look at the big picture and try to understand what the current step is doing to contribute to this big picture. This may clarify what's being asked!\n",
    "1. These questions intentionally build off of each other and the final result matters! In fact, you can 'get a question correct', but only receive partial credit on it because a previous answer was wrong.\n",
    "    - Credit for a question will typically receive partial credit based on *how close* your answer is to correct (as well as some credit for a solution in the correct form). \n",
    "    - You should try to assess your answer to each question based on what you understand of the data. This might involve writing extensive code (that isn't turned in) just to check your work! Suggestions on checking your work are given in the assignment, but you should also think of your own ways of checking your work.\n",
    "    - As you do this project, think about the data from the perspective of the student (which should be easy to do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fp = os.path.join('data', 'grades.csv')\n",
    "grades = pd.read_csv(grades_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started: enumerating the assignments\n",
    "\n",
    "First, you will list all the 'assignment names' and what part of the syllabus to which they belong.\n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "Create a function `get_assignment_names` that takes in a dataframe like `grades` and returns a dictionary with the following structure:\n",
    "- The keys are the general areas of the syllabus: `lab, project, midterm, final, disc, checkpoint`\n",
    "- The values are lists that contain the assignment names of that type. For example the lab assignments all have names of the form `labXX` where `XX` is a zero-padded two digit number. See the doctests for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment_names(df):\n",
    "    \n",
    "    names = {'lab':[], 'project': [], 'midterm':['Midterm'], 'final':['Final'], 'disc':[], 'checkpoint':[]}\n",
    "    \n",
    "    \n",
    "    #is it always up to 10?\n",
    "    for col in df.columns:\n",
    "        if ('lab' in col) and (len(col) == 5):\n",
    "            names['lab'].append(col)\n",
    "        elif ('project' in col) and (len(col) == 9):\n",
    "            names['project'].append(col)\n",
    "        elif ('discussion' in col) and (len(col) == 12):\n",
    "            names['disc'].append(col)\n",
    "        elif ('checkpoint' in col) and (len(col) == 22):\n",
    "            names['checkpoint'].append(col)\n",
    "            \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PID', 'College', 'Level', 'lab01', 'lab01 - Max Points',\n",
       "       'lab01 - Lateness (H:M:S)', 'lab02', 'lab02 - Max Points',\n",
       "       'lab02 - Lateness (H:M:S)', 'project01', 'project01 - Max Points',\n",
       "       'project01 - Lateness (H:M:S)', 'lab03', 'lab03 - Max Points',\n",
       "       'lab03 - Lateness (H:M:S)', 'project01_free_response',\n",
       "       'project01_free_response - Max Points',\n",
       "       'project01_free_response - Lateness (H:M:S)', 'lab04',\n",
       "       'lab04 - Max Points', 'lab04 - Lateness (H:M:S)', 'lab05',\n",
       "       'lab05 - Max Points', 'lab05 - Lateness (H:M:S)',\n",
       "       'project02_checkpoint01', 'project02_checkpoint01 - Max Points',\n",
       "       'project02_checkpoint01 - Lateness (H:M:S)', 'Midterm',\n",
       "       'Midterm - Max Points', 'Midterm - Lateness (H:M:S)', 'lab06',\n",
       "       'lab06 - Max Points', 'lab06 - Lateness (H:M:S)',\n",
       "       'project02_checkpoint02', 'project02_checkpoint02 - Max Points',\n",
       "       'project02_checkpoint02 - Lateness (H:M:S)', 'lab07',\n",
       "       'lab07 - Max Points', 'lab07 - Lateness (H:M:S)', 'project02',\n",
       "       'project02 - Max Points', 'project02 - Lateness (H:M:S)',\n",
       "       'project02_free_response', 'project02_free_response - Max Points',\n",
       "       'project02_free_response - Lateness (H:M:S)', 'lab08',\n",
       "       'lab08 - Max Points', 'lab08 - Lateness (H:M:S)', 'lab09',\n",
       "       'lab09 - Max Points', 'lab09 - Lateness (H:M:S)',\n",
       "       'project03_checkpoint01', 'project03_checkpoint01 - Max Points',\n",
       "       'project03_checkpoint01 - Lateness (H:M:S)', 'project03',\n",
       "       'project03 - Max Points', 'project03 - Lateness (H:M:S)', 'Final',\n",
       "       'Final - Max Points', 'Final - Lateness (H:M:S)',\n",
       "       'Total Lateness (H:M:S)', 'project05_free_response',\n",
       "       'project05_free_response - Max Points',\n",
       "       'project05_free_response - Lateness (H:M:S)', 'project04',\n",
       "       'project04 - Max Points', 'project04 - Lateness (H:M:S)', 'project05',\n",
       "       'project05 - Max Points', 'project05 - Lateness (H:M:S)',\n",
       "       'discussion01', 'discussion01 - Max Points',\n",
       "       'discussion01 - Lateness (H:M:S)', 'discussion02',\n",
       "       'discussion02 - Max Points', 'discussion02 - Lateness (H:M:S)',\n",
       "       'discussion03', 'discussion03 - Max Points',\n",
       "       'discussion03 - Lateness (H:M:S)', 'discussion04',\n",
       "       'discussion04 - Max Points', 'discussion04 - Lateness (H:M:S)',\n",
       "       'discussion05', 'discussion05 - Max Points',\n",
       "       'discussion05 - Lateness (H:M:S)', 'discussion06',\n",
       "       'discussion06 - Max Points', 'discussion06 - Lateness (H:M:S)',\n",
       "       'discussion07', 'discussion07 - Max Points',\n",
       "       'discussion07 - Lateness (H:M:S)', 'discussion08',\n",
       "       'discussion08 - Max Points', 'discussion08 - Lateness (H:M:S)',\n",
       "       'discussion09', 'discussion09 - Max Points',\n",
       "       'discussion09 - Lateness (H:M:S)', 'discussion10',\n",
       "       'discussion10 - Max Points', 'discussion10 - Lateness (H:M:S)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(grades.loc[64].project01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.loc[62].project01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing project grades\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Compute the total score for the project portion of the course according to the syllabus. Create a function `projects_total` that takes in `grades` and computes the total project grade for the quarter according to the syllabus. The output Series should contain values between 0 and 1.\n",
    "\n",
    "*Note*: Don't forget to properly handle students who didn't turn in assignments! (Use your experience and common sense).\n",
    "\n",
    "*Note:* To check your work, try (1) calculating the score for a few types of students by hand, and (2) calculate the statistics for the class performance on each individual course project, making sure they look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to handle students who don't turn it in\n",
    "def projects_total(grades):\n",
    "    \n",
    "    names = get_assignment_names(grades)\n",
    "\n",
    "    copy = grades.copy()\n",
    "    project_scores = [] \n",
    "    \n",
    "    #loop thru projects\n",
    "    for project_name in names['project']:\n",
    "        \n",
    "        #check if theres a free response\n",
    "        if project_name + '_free_response' in copy.columns:\n",
    "            project_fr_name = project_name + '_free_response'\n",
    "        else:\n",
    "            project_fr_name = None\n",
    "            \n",
    "        #handle NaN values to get 0 score\n",
    "        copy[project_name] = copy[project_name].fillna(0)\n",
    "        if project_fr_name != None: #free response exists\n",
    "            copy[project_fr_name] = copy[project_fr_name].fillna(0)\n",
    "            \n",
    "            \n",
    "        #numerator\n",
    "        if project_fr_name != None: #free response exists\n",
    "            points = copy[project_name] + copy[project_fr_name]\n",
    "        else:\n",
    "            points = copy[project_name]\n",
    "        \n",
    "        #denominator\n",
    "        if project_fr_name != None: #free response exists\n",
    "            total_points = copy[project_name + ' - Max Points'] + copy[project_fr_name + ' - Max Points']\n",
    "        else:\n",
    "            total_points = copy[project_name + ' - Max Points']\n",
    "            \n",
    "        #compute scores for this project\n",
    "        score = points/total_points\n",
    "        project_scores.append(score)\n",
    "        \n",
    "    \n",
    "    total_scores = sum(project_scores) / len(names['project']) #divided by number of projects\n",
    "    return total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.900000\n",
       "1      0.759333\n",
       "2      0.673333\n",
       "3      0.952667\n",
       "4      0.718667\n",
       "         ...   \n",
       "530    0.949333\n",
       "531    0.846667\n",
       "532    0.837333\n",
       "533    0.797333\n",
       "534    0.948000\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects_total(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = projects_total(grades)\n",
    "#np.all((0 <= out) & (out <= 1))\n",
    "\n",
    "0.7 < out.mean() < 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing lab grades\n",
    "\n",
    "Now, you will clean and process the lab grades, which is a little more complicated. To do this, you will develop functions that:\n",
    "- 'normalize' the grades, \n",
    "- adjust for late submissions, \n",
    "- drop the lowest lab grade, and \n",
    "- creates a total lab score for each student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Unfortunately, Gradescope sometimes experiences a delay in registering when an assignment is submitted during \"periods of heavy usage\" (i.e. near a submission deadline). You need to assess when a student's assignment was actually turned in on time, even if Gradescope did not process it in time. To do this, it is helpful to know:\n",
    "* Every late submission has to be submitted by a TA (late submissions are turned off).\n",
    "* TAs never submitted a late assignment \"just after\" the deadline. \n",
    "* The deadlines were at midnight and students had to come to staff hours to late-submit their assignment.\n",
    "\n",
    "Create a function `last_minute_submissions` that takes in the dataframe `grades` and outputs the number of submissions on each assignment that were turned in on time by the student, yet marked 'late' by Gradescope. See the doctest for more details.\n",
    "\n",
    "*Note:* You have to figure out what truly is a late submission by looking at the data and understanding the facts about the data generating process above. There is some ambiguity in finding which submissions are truly late; you will *make a best guess for a threshold* by looking at this dataset. This question is about 'cleaning' a messy 'data recording process'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_minute_submissions(grades):\n",
    "    \n",
    "    late_tag = ' - Lateness (H:M:S)'\n",
    "        \n",
    "    names = get_assignment_names(grades)\n",
    "    \n",
    "    late_submissions = {}\n",
    "    \n",
    "    for lab in names['lab']:\n",
    "        \n",
    "        gradescope_late = grades.copy()\n",
    "        \n",
    "        gradescope_late = gradescope_late[gradescope_late[lab + late_tag] != '00:00:00']\n",
    "    \n",
    "        gradescope_late[lab + late_tag] = gradescope_late[lab + late_tag].str.slice(stop = 2)\n",
    "                \n",
    "        gradescope_late[lab + late_tag] = gradescope_late[lab + late_tag].astype(int)\n",
    "        \n",
    "        error_late = gradescope_late.loc[gradescope_late[lab + late_tag] < 9]\n",
    "        \n",
    "        late_submissions[lab] = len(error_late)\n",
    "        \n",
    "    series = pd.Series(late_submissions)  \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'grades.csv')\n",
    "grades = pd.read_csv(fp)\n",
    "out = last_minute_submissions(grades)\n",
    "(out > 0).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Now you need to adjust the lab grades for late submissions -- however, you need to take into account your investigation in the previous question, since students shouldn't be penalized by a bug in Gradescope!\n",
    "\n",
    "Create a function `lateness_penalty` that takes in a 'Lateness' column and returns a column of penalties (represented by the values `1.0,0.9,0.8,0.5` according to the syllabus). Only *truly* late submissions should be counted as late.\n",
    "\n",
    "*Note*: For the purpose of this project, we will only be calculating lateness for labs. There is no penalty for lateness for projects, discussions, nor checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lateness_penalty(col):\n",
    "    \n",
    "    #make series, starting with all 1.0 penalties\n",
    "    penalties = pd.Series(1.0, index = col.index)\n",
    "\n",
    "    #convert times to numbers\n",
    "    hours = col.str.slice(stop = 2)\n",
    "    hours = hour.astype(int)\n",
    "\n",
    "    #late 1 week (before 9 hours doesnt count)\n",
    "    week_late = hours[hours.between(9, 168)]\n",
    "    week_late_indices = week_late.index.values\n",
    "    penalties[week_late_indices] = 0.9\n",
    "\n",
    "    #late 2 weeks\n",
    "    two_weeks = hours[hours.between(169, 336)]\n",
    "    two_weeks_indices = two_weeks.index.values\n",
    "    penalties[two_weeks_indices] = 0.8\n",
    "\n",
    "    #beyond\n",
    "    beyond = hours[hours >= 337]\n",
    "    beyond_indices = beyond.index.values\n",
    "    penalties[beyond_indices] = 0.5\n",
    "    \n",
    "    return penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'grades.csv')\n",
    "col = pd.read_csv(fp)['lab01 - Lateness (H:M:S)']\n",
    "out = lateness_penalty(col)\n",
    "set(out.unique()) <= {1.0, 0.9, 0.8, 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 week (168 hrs) - 10%\n",
    "2 weeks (169-336 hrs) - 20%\n",
    "beyond (337+hrs) - 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "      ... \n",
       "530    0.9\n",
       "531    1.0\n",
       "532    1.0\n",
       "533    1.0\n",
       "534    1.0\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = grades['lab01 - Lateness (H:M:S)']\n",
    "\n",
    "#make series, starting with all 1.0 penalties\n",
    "penalties = pd.Series(1.0, index = col.index)\n",
    "\n",
    "#convert times to numbers\n",
    "hours = col.str.slice(stop = 2)\n",
    "hours = hour.astype(int)\n",
    "\n",
    "#late 1 week (before 9 hours doesnt count)\n",
    "week_late = hours[hours.between(9, 168)]\n",
    "week_late_indices = week_late.index.values\n",
    "penalties[week_late_indices] = 0.9\n",
    "\n",
    "#late 2 weeks\n",
    "two_weeks = hours[hours.between(169, 336)]\n",
    "two_weeks_indices = two_weeks.index.values\n",
    "penalties[two_weeks_indices] = 0.8\n",
    "\n",
    "#beyond\n",
    "beyond = hours[hours >= 337]\n",
    "beyond_indices = beyond.index.values\n",
    "penalties[beyond_indices] = 0.5\n",
    "\n",
    "penalties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     10\n",
       "1    100\n",
       "2    100\n",
       "3     10\n",
       "4     10\n",
       "5    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col = pd.Series([10, 10, 10, 10, 10, 10], index = [0, 1, 2, 3, 4, 5])\n",
    "i_to_replace = np.array([1, 2, 5])\n",
    "replaced = 100\n",
    "test_col[i_to_replace] = 100\n",
    "test_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Create a function `process_labs` that takes in a dataframe like `grades` and returns a dataframe of processed lab scores. The output should:\n",
    "* share the same index as `grades`,\n",
    "* have columns given by the lab assignment names (e.g. `lab01,...lab10`)\n",
    "* have values representing the lab grades for each assignment, adjusted for Lateness and scaled to a score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Create a function `lab_total` that takes in dataframe of processed assignments (like the output of Question 5) and computes the total lab grade for each student according to the syllabus (returning a Series). Your answers should be proportions between 0 and 1. For example, if there are only 3 labs, and a student received scores of {80%,90%,100%}, then the total score would be 0.95.\n",
    "\n",
    "*Note*: Don't forget to properly handle students who didn't turn in assignments! (Use your experience and common sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Finally, you need to create the final course grades. To do this, you will add up the total of each course component according to the weights given in the syllabus. \n",
    "\n",
    "* Create a function `total_points` that takes in `grades` and returns the final course grades according to the syllabus. Course grades should be proportions between zero and one.\n",
    "* Create a function `final_grades` that takes in the final course grades as above and returns a Series of letter grades given by the standard cutoffs (`A >= .90`, `.90 > B >= .80`, `.80 > C >= .70`, `.70 > D >= .60`, `.60 > F`). You should not use rounding to determining the letter grades.\n",
    "* Create a function `letter_proportions` which takes in the dataframe `grades` and outputs a Series that contains the proportion of the class that received each grade. (This question requires you to put everything together).\n",
    "* The indices should be ordered by the proportion of the class that receives that grade, from largest to smallest.\n",
    "\n",
    "*Note 1*: Don't repeat yourself when computing the checkpoint and discussion portions of the course.\n",
    "\n",
    "*Note 2*: Only the lab portion of the course accounts for late assignments; you may assume all assignments in other portions are turned in without penalty.\n",
    "\n",
    "*Note 3*: These values should add up to exactly 1.0. If you are getting something close such as 0.99999, that means there is a slight issue with your code from above. \n",
    "\n",
    "To check your work, verify the course grade distribution and relevant statistics! Do the work by hand for a few students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Sophomores get better grades?\n",
    "\n",
    "**Question 8**\n",
    "\n",
    "You notice that students who are sophomores on average did better in the class (if you can't verify this, you should go back and check your work!). Is this difference significant, or just due to noise?\n",
    "\n",
    "Perform a hypothesis test, assessing likelihood of the null hypothesis: \n",
    "> \"sophomores earn grades that are roughly equal on average to the rest of the class.\"\n",
    "\n",
    "\n",
    "Create a function `simulate_pval` which takes in the number of simulations `N` and `grades` and returns the the likelihood that the grade of sophomores was no better on average than the class as a whole (i.e. calculate the p-value).\n",
    "\n",
    "*Note:* To check your work, plot the sampling distribution and the observation. Do these values look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the true distribution of grades?\n",
    "\n",
    "The gradebook for this class only reflects one particular instance of each student's performance, subject to the effects of all the little events and hiccups that occurred throughout the quarter. Might you have done better on the midterm had your roommate kept you up all night with their coughing? Wasn't it lucky that the example you were studying just before the final happened to appear on the exam?\n",
    "\n",
    "**Question 9**\n",
    "\n",
    "This question will simulate these '(un)lucky, random events' by adding or subtracting random amounts to each assignment before calculating the final grades. These 'random amounts' will be drawn from a Gaussian distribution of mean 0 and a std deviation 0.02:\n",
    "```\n",
    "np.random.normal(0, 0.02, size=(num_rows, num_cols))\n",
    "```\n",
    "Intuitively, such a model says that random events may bump up or down a given grade (given as a proportion):\n",
    "- which on average has no effect on the class as a whole (mean 0),\n",
    "- which not uncommonly might perturb a grade by 2% (std dev 0.02).\n",
    "\n",
    "Create a function `total_points_with_noise` that takes in a dataframe like `grades`, adds noise to the assignments as described above, and returns the final scores using *the same procedure* as questions 1-7.\n",
    "\n",
    "*Note:* You should be able to reuse (or minorly change) the code from previous problems. Try to be DRY (don't repeat yourself)!\n",
    "\n",
    "*Note 1:* Once adding the noise to the assignment scores, use the `np.clip` function to be sure each assignment retains a score between 0% and 100%.\n",
    "\n",
    "*Note 2:* To check your work -- what would you expect the difference between the actual scores and noisy scores to be, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-answer questions (hard-coded)\n",
    "\n",
    "Use your functions from above to understanding the data and answer the following questions. The function below should return **hard-coded values**. It should not compute anything!\n",
    "\n",
    "**Question 10**\n",
    "\n",
    "Create a function `short_answer` of zero variables that returns (hard-coded) answers to the following question in a list:\n",
    "0. For the class on average, what is the difference between students' scores (`total_points`) and their scores with noise (`total_points_with_noise`)? (Remark: plot the distribution of differences; does this align with what you know about binomial distributions?)\n",
    "1. What percentage of the class only sees their grade change at most (but not including) $\\pm 0.01$?\n",
    "2. What is the 95% confidence interval for the statistic above? (see [DSC10](https://www.inferentialthinking.com/chapters/13/3/Confidence_Intervals.html) and use `np.percentile`)\n",
    "3. What proportion of the class sees a change in their letter grade?\n",
    "4. The assumption behind the model in Question 9 is that:\n",
    "    - The (observed) gradebook well represents the true population of students,\n",
    "    - The noisy scores represent other possible observations drawn from the true population of students.\n",
    "    - Answer `True` or `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you finished the project!\n",
    "\n",
    "### Before you submit:\n",
    "* Be sure you run the doctests on all your code in project01.py\n",
    "\n",
    "### To submit:\n",
    "* **Upload the .py file to gradescope**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
